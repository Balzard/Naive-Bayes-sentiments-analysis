# Unigram and bigram language models, Naive Bayes classifier
2021 first project of Computational linguistics course of UCLouvain.

## Corpus
The corpus comes from the Stanford Sentiment Treebank. This contains sentences which have been annotated to associate a sentiment score.

<ul>
  <li>0 (strongly negative)</li>
  <li>1 (mildly negative)</li>
  <li>2 (neutral)</li>
  <li>3 (mildly positive)</li>
  <li>4 (strongly positibe)</li>
</ul>

## Part 1
Build voc (word occuring >= 3 times) and report the last 10 alphabetic word of the voc

## Part 2
Learn a bigram language Maximum Likelihood Estimate model and a Laplace smoothed bigram model.
Compute the test set perplexity

## Part 3
Implementation of an unigram Naive Bayes classifier

## Part 4
Adaptation of Part 3 to make a Binary Naive Bayes model.
Adaptation of Part 3 to take negative words into account.
