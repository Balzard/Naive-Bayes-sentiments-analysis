{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import BracketParseCorpusReader\n",
    "from nltk.util import bigrams\n",
    "from nltk.lm import MLE, Vocabulary, Laplace\n",
    "from nltk.lm.preprocessing import pad_both_ends, flatten\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = BracketParseCorpusReader(root=\"\", fileids=[\"p1_train_StephLeblanc.txt\"])\n",
    "test_corpus = BracketParseCorpusReader(root=\"\", fileids=[\"p1_test_StephLeblanc.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'warm', ',', 'funny', ',', 'engaging', 'film', ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = corpus.words()\n",
    "words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary(words,unk_cutoff=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('written', 8),\n",
       " ('wrong', 7),\n",
       " ('wry', 3),\n",
       " ('year', 24),\n",
       " ('years', 12),\n",
       " ('yet', 20),\n",
       " ('you', 180),\n",
       " ('young', 14),\n",
       " ('your', 34),\n",
       " ('zeal', 3)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last10 = sorted(vocab)[-10:]\n",
    "last10 = [(last10[i], vocab[last10[i]]) for i in range(len(last10))]\n",
    "last10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<UNK>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_voc = Vocabulary(vocab.counts,unk_cutoff=1)\n",
    "all_voc.lookup(\"<s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.57521672616012"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_rate = (1-len(vocab)/len(all_voc)) * 100\n",
    "oov_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence(corpus:BracketParseCorpusReader):\n",
    "    sentences = list(corpus.tagged_sents())\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = [j[0] for j in sentences[i]]\n",
    "        \n",
    "    text = list(flatten(pad_both_ends(sent, n=2) for sent in sentences))\n",
    "    return text\n",
    "\n",
    "def get_sentences(corpus:BracketParseCorpusReader):\n",
    "    sentences = list(corpus.tagged_sents())\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = [j[0] for j in sentences[i]]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1759"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = get_sequence(corpus)\n",
    "bigram = list(list(bigrams(pad_both_ends(sent, n=2))) for sent in get_sentences(corpus))\n",
    "v = Vocabulary(text, unk_cutoff=3)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle = MLE(2)\n",
    "mle.fit(bigram, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 0.2615, 'The': 0.1235, 'A': 0.091, 'It': 0.0715, 'This': 0.0315}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_mle = {}\n",
    "for word in v:\n",
    "    score_mle[word] = mle.score(word,[\"<s>\"])\n",
    "dict(Counter(score_mle).most_common(5))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1235"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle.score(\"The\",[\"<s>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9840510366826156"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle.score(\"</s>\",\".\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplace = Laplace(2)\n",
    "laplace.fit(bigram,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 0.1392876129718235,\n",
       " 'The': 0.0659223817118554,\n",
       " 'A': 0.04864433811802233,\n",
       " 'It': 0.03827751196172249,\n",
       " 'This': 0.01701222753854333}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_laplace = {}\n",
    "for word in v:\n",
    "    score_laplace[word] = laplace.score(word,[\"<s>\"])\n",
    "dict(Counter(score_laplace).most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005675368898978433"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laplace.score(\"aaaaa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165.21112109883478"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test set perplexity\n",
    "test_padded = list(list(pad_both_ends(sent, n=2)) for sent in get_sentences(test_corpus))\n",
    "test_padded = [list(bigrams(i)) for i in test_padded]\n",
    "m = sum([len(i) for i in test_padded])\n",
    "\n",
    "sum_prob = sum([math.log2(laplace.score(j[1], [j[0]])) for i in test_padded for j in i])\n",
    "test_set_perplexity = 2**((-1/m)*(sum_prob))\n",
    "test_set_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4162"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(i) for i in test_padded])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3\n",
    "## < 2 => negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words = {k:v for k,v in vocab.counts.items() if v >= vocab.cutoff}\n",
    "bigdoc = {k:[] for k in [0,1,2,3,4]}\n",
    "bigdoc[1] = list(flatten([line.leaves() for line in corpus.parsed_sents() if int(line.label()) == 1]))\n",
    "sents = corpus.parsed_sents()\n",
    "list(flatten([line.leaves() for line in sents if int(line.label()) == 0])).count(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bayes(doc:BracketParseCorpusReader, classes:list):\n",
    "    \n",
    "    sents = doc.parsed_sents()\n",
    "    bigdoc = {k:[] for k in classes}\n",
    "    n_doc = len(sents)\n",
    "    voc = Vocabulary(doc.words(), unk_cutoff=3)\n",
    "    # only keep words with frequency >= 3 as feature\n",
    "    bag_of_words = {k:v for k,v in voc.counts.items() if v >= voc.cutoff}\n",
    "    prior = {k:0 for k in classes}\n",
    "    loglikelihood = {}\n",
    "    \n",
    "    for c in classes:\n",
    "        n_c = len([1 for i in sents if int(i.label()) == c])\n",
    "        prior[c] = math.log(n_c/n_doc)\n",
    "        bigdoc[c] = list(flatten([line.leaves() for line in sents if int(line.label()) == c]))\n",
    "        somme = sum([bigdoc[c].count(w) + 1 for w in bag_of_words.keys()])\n",
    "        \n",
    "        for word in bag_of_words.keys():\n",
    "            count = bigdoc[c].count(word)\n",
    "            loglikelihood[(word,c)] = math.log((count + 1)/ somme) #sum([bigdoc[c].count(w) + 1 for w in bag_of_words.keys()])\n",
    "            \n",
    "    return prior, loglikelihood, bag_of_words\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bayes(sentence:list, prior:dict, loglikelihood:dict, classes:list, voc:dict):\n",
    "    somme = {}\n",
    "    for c in classes:\n",
    "        somme[c] = prior[c]\n",
    "        for word in sentence:\n",
    "            if word in voc.keys():\n",
    "                somme[c] += loglikelihood[(word,c)]\n",
    "    pred =  max(somme, key=somme.get)\n",
    "    return pred >= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior, loglik, v = train_bayes(corpus, [0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bayes(test_corpus.sents()[1], prior, loglik, [0,1,2,3,4], v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corpus.parsed_sents()[0].label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(voc, test_set, prior, loglik, classes):\n",
    "    predictions = 0\n",
    "    success = 0\n",
    "    parsed_sents = test_set.parsed_sents()\n",
    "    sents = test_set.sents()\n",
    "    for i in range(len(sents)):\n",
    "        predict = test_bayes(sents[i],prior, loglik, classes, voc)\n",
    "        predictions += 1\n",
    "        label = 1 if int(parsed_sents[i].label()) >= 2 else 0\n",
    "            \n",
    "        if predict == label:\n",
    "            success += 1\n",
    "    return success/predictions*100\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(v, test_corpus, prior, loglik, [0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4\n",
    "remove duplicates in each sentence for question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_corpus = list(flatten([list(set(i)) for i in corpus.sents()]))\n",
    "clipped_test_corpus = list(flatten([list(set(i)) for i in test_corpus.sents()]))\n",
    "v = Vocabulary(clipped_corpus,unk_cutoff=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_bayes_clip(doc:BracketParseCorpusReader, classes:list):\n",
    "    \n",
    "    sents = doc.parsed_sents()\n",
    "    bigdoc = {k:[] for k in classes}\n",
    "    n_doc = len(sents)\n",
    "    clipped_corpus = list(flatten([list(set(i)) for i in doc.sents()]))\n",
    "    voc = Vocabulary(clipped_corpus, unk_cutoff=3)\n",
    "    # only keep words with frequency >= 3 as feature\n",
    "    bag_of_words = {k:v for k,v in voc.counts.items() if v >= voc.cutoff}\n",
    "    prior = {k:0 for k in classes}\n",
    "    loglikelihood = {}\n",
    "    \n",
    "    for c in classes:\n",
    "        n_c = len([1 for i in sents if int(i.label()) == c])\n",
    "        prior[c] = math.log(n_c/n_doc)\n",
    "        bigdoc[c] = list(flatten([line.leaves() for line in sents if int(line.label()) == c]))\n",
    "        somme = sum([bigdoc[c].count(w) + 1 for w in bag_of_words.keys()])\n",
    "        \n",
    "        for word in bag_of_words.keys():\n",
    "            count = bigdoc[c].count(word)\n",
    "            loglikelihood[(word,c)] = math.log((count + 1)/ somme) #sum([bigdoc[c].count(w) + 1 for w in bag_of_words.keys()])\n",
    "            \n",
    "    return prior, loglikelihood, bag_of_words\n",
    "\n",
    "def test_bayes_clip(sentence:list, prior:dict, loglikelihood:dict, classes:list, voc:dict):\n",
    "    somme = {}\n",
    "    for c in classes:\n",
    "        somme[c] = prior[c]\n",
    "        for word in sentence:\n",
    "            if word in voc.keys():\n",
    "                somme[c] += loglikelihood[(word,c)]\n",
    "    pred =  max(somme, key=somme.get)\n",
    "    return pred >= 2\n",
    "\n",
    "def accuracy_clip(voc, test_set, prior, loglik, classes):\n",
    "    predictions = 0\n",
    "    success = 0\n",
    "    parsed_sents = test_set.parsed_sents()\n",
    "    sents = [list(set(i)) for i in test_set.sents()]\n",
    "    for i in range(len(sents)):\n",
    "        predict = test_bayes(sents[i],prior, loglik, classes, voc)\n",
    "        predictions += 1\n",
    "        label = 1 if int(parsed_sents[i].label()) >= 2 else 0\n",
    "            \n",
    "        if predict == label:\n",
    "            success += 1\n",
    "    return success/predictions*100\n",
    "\n",
    "prior_clip, loglik_clip, v_clip = train_bayes_clip(corpus, [0,1,2,3,4])\n",
    "accuracy_clip(v_clip, test_corpus, prior_clip, loglik_clip, [0,1,2,3,4])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = [\"n't\", \"not\", \"no\", \"never\"]\n",
    "punc = ['.', ',', ':', '?', '!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_words_process(sent: list):\n",
    "    \"\"\" Prepend the prefix NOT_ to every word after a token of logical negation till\n",
    "        the next punctuation mark.\n",
    "\n",
    "    Args:\n",
    "        sent (list): list of words\n",
    "\n",
    "    Returns:\n",
    "        list: sent with modified words\n",
    "    \"\"\"\n",
    "    tmp,i = 0, 0\n",
    "    while i < len(sent):\n",
    "        if sent[i].lower() in neg:\n",
    "            tmp = i + 1\n",
    "            try:\n",
    "                while(sent[tmp]) not in punc:\n",
    "                    sent[tmp] = f\"NOT_{sent[tmp]}\"\n",
    "                    tmp += 1\n",
    "                i = tmp\n",
    "            except IndexError:\n",
    "                break\n",
    "        else:\n",
    "            i += 1\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_bayes_neg(doc:BracketParseCorpusReader, classes:list):\n",
    "    \n",
    "    sents = doc.parsed_sents()\n",
    "    bigdoc = {k:[] for k in classes}\n",
    "    n_doc = len(sents)\n",
    "    clipped_corpus = list(flatten([neg_words_process(i) for i in doc.sents()]))\n",
    "    voc = Vocabulary(clipped_corpus, unk_cutoff=3)\n",
    "    # only keep words with frequency >= 3 as feature\n",
    "    bag_of_words = {k:v for k,v in voc.counts.items() if v >= voc.cutoff}\n",
    "    prior = {k:0 for k in classes}\n",
    "    loglikelihood = {}\n",
    "    \n",
    "    for c in classes:\n",
    "        n_c = len([1 for i in sents if int(i.label()) == c])\n",
    "        prior[c] = math.log(n_c/n_doc)\n",
    "        bigdoc[c] = list(flatten([line.leaves() for line in sents if int(line.label()) == c]))\n",
    "        somme = sum([bigdoc[c].count(w) + 1 for w in bag_of_words.keys()])\n",
    "        \n",
    "        for word in bag_of_words.keys():\n",
    "            count = bigdoc[c].count(word)\n",
    "            loglikelihood[(word,c)] = math.log((count + 1)/ somme) #sum([bigdoc[c].count(w) + 1 for w in bag_of_words.keys()])\n",
    "            \n",
    "    return prior, loglikelihood, bag_of_words\n",
    "\n",
    "def test_bayes_neg(sentence:list, prior:dict, loglikelihood:dict, classes:list, voc:dict):\n",
    "    somme = {}\n",
    "    for c in classes:\n",
    "        somme[c] = prior[c]\n",
    "        for word in sentence:\n",
    "            if word in voc.keys():\n",
    "                somme[c] += loglikelihood[(word,c)]\n",
    "    pred =  max(somme, key=somme.get)\n",
    "    return pred >= 2\n",
    "\n",
    "def accuracy_neg(voc, test_set, prior, loglik, classes):\n",
    "    predictions = 0\n",
    "    success = 0\n",
    "    parsed_sents = test_set.parsed_sents()\n",
    "    sents = [list(set(i)) for i in test_set.sents()]\n",
    "    for i in range(len(sents)):\n",
    "        predict = test_bayes(sents[i],prior, loglik, classes, voc)\n",
    "        predictions += 1\n",
    "        label = 1 if int(parsed_sents[i].label()) >= 2 else 0\n",
    "            \n",
    "        if predict == label:\n",
    "            success += 1\n",
    "    return success/predictions*100\n",
    "\n",
    "prior_neg, loglik_neg, v_neg = train_bayes_neg(corpus, [0,1,2,3,4])\n",
    "accuracy_neg(v_clip, test_corpus, prior_clip, loglik_clip, [0,1,2,3,4])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'warm',\n",
       " ',',\n",
       " 'funny',\n",
       " ',',\n",
       " 'engaging',\n",
       " 'film',\n",
       " '.',\n",
       " 'The',\n",
       " 'band',\n",
       " \"'s\",\n",
       " 'courage',\n",
       " 'in',\n",
       " 'the',\n",
       " 'face',\n",
       " 'of',\n",
       " 'official',\n",
       " 'repression',\n",
       " 'is',\n",
       " 'inspiring',\n",
       " ',',\n",
       " 'especially',\n",
       " 'for',\n",
       " 'aging',\n",
       " 'hippies',\n",
       " '-LRB-',\n",
       " 'this',\n",
       " 'one',\n",
       " 'included',\n",
       " '-RRB-',\n",
       " '.',\n",
       " 'Not',\n",
       " 'NOT_only',\n",
       " 'NOT_is',\n",
       " 'NOT_Undercover',\n",
       " 'NOT_Brother',\n",
       " 'NOT_as',\n",
       " 'NOT_funny',\n",
       " ',',\n",
       " 'if',\n",
       " 'not',\n",
       " 'NOT_more',\n",
       " 'NOT_so',\n",
       " ',',\n",
       " 'than',\n",
       " 'both',\n",
       " 'Austin',\n",
       " 'Powers',\n",
       " 'films',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'also',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'smarter',\n",
       " ',',\n",
       " 'savvier',\n",
       " 'spoofs',\n",
       " 'to',\n",
       " 'come',\n",
       " 'along',\n",
       " 'in',\n",
       " 'some',\n",
       " 'time',\n",
       " '.',\n",
       " 'Woody',\n",
       " 'Allen',\n",
       " \"'s\",\n",
       " 'latest',\n",
       " 'is',\n",
       " 'an',\n",
       " 'ambling',\n",
       " ',',\n",
       " 'broad',\n",
       " 'comedy',\n",
       " 'about',\n",
       " 'all',\n",
       " 'there',\n",
       " 'is',\n",
       " 'to',\n",
       " 'love',\n",
       " '--',\n",
       " 'and',\n",
       " 'hate',\n",
       " '--',\n",
       " 'about',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'biz',\n",
       " '.',\n",
       " 'The',\n",
       " 'inhospitability',\n",
       " 'of',\n",
       " 'the',\n",
       " 'land',\n",
       " 'emphasizes',\n",
       " 'the',\n",
       " 'spare',\n",
       " 'precision',\n",
       " 'of',\n",
       " 'the',\n",
       " 'narratives',\n",
       " 'and',\n",
       " 'helps',\n",
       " 'to',\n",
       " 'give',\n",
       " 'them',\n",
       " 'an',\n",
       " 'atavistic',\n",
       " 'power',\n",
       " ',',\n",
       " 'as',\n",
       " 'if',\n",
       " 'they',\n",
       " 'were',\n",
       " 'tales',\n",
       " 'that',\n",
       " 'had',\n",
       " 'been',\n",
       " 'handed',\n",
       " 'down',\n",
       " 'since',\n",
       " 'the',\n",
       " 'beginning',\n",
       " 'of',\n",
       " 'time',\n",
       " '.',\n",
       " 'Like',\n",
       " 'Mike',\n",
       " 'is',\n",
       " 'a',\n",
       " 'winner',\n",
       " 'for',\n",
       " 'kids',\n",
       " ',',\n",
       " 'and',\n",
       " 'no',\n",
       " 'NOT_doubt',\n",
       " 'NOT_a',\n",
       " 'NOT_winner',\n",
       " 'NOT_for',\n",
       " 'NOT_Lil',\n",
       " 'NOT_Bow',\n",
       " 'NOT_Wow',\n",
       " ',',\n",
       " 'who',\n",
       " 'can',\n",
       " 'now',\n",
       " 'add',\n",
       " 'movies',\n",
       " 'to',\n",
       " 'the',\n",
       " 'list',\n",
       " 'of',\n",
       " 'things',\n",
       " 'he',\n",
       " 'does',\n",
       " 'well',\n",
       " '.',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'coming-of-age',\n",
       " 'story',\n",
       " 'we',\n",
       " \"'ve\",\n",
       " 'all',\n",
       " 'seen',\n",
       " 'bits',\n",
       " 'of',\n",
       " 'in',\n",
       " 'other',\n",
       " 'films',\n",
       " '--',\n",
       " 'but',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'rarely',\n",
       " 'been',\n",
       " 'told',\n",
       " 'with',\n",
       " 'such',\n",
       " 'affecting',\n",
       " 'grace',\n",
       " 'and',\n",
       " 'cultural',\n",
       " 'specificity',\n",
       " '.',\n",
       " 'A',\n",
       " 'rigorously',\n",
       " 'structured',\n",
       " 'and',\n",
       " 'exquisitely',\n",
       " 'filmed',\n",
       " 'drama',\n",
       " 'about',\n",
       " 'a',\n",
       " 'father',\n",
       " 'and',\n",
       " 'son',\n",
       " 'connection',\n",
       " 'that',\n",
       " 'is',\n",
       " 'a',\n",
       " 'brief',\n",
       " 'shooting',\n",
       " 'star',\n",
       " 'of',\n",
       " 'love',\n",
       " '.',\n",
       " 'Huston',\n",
       " 'nails',\n",
       " 'both',\n",
       " 'the',\n",
       " 'glad-handing',\n",
       " 'and',\n",
       " 'the',\n",
       " 'choking',\n",
       " 'sense',\n",
       " 'of',\n",
       " 'hollow',\n",
       " 'despair',\n",
       " '.',\n",
       " 'may',\n",
       " 'not',\n",
       " 'NOT_have',\n",
       " 'NOT_generated',\n",
       " 'NOT_many',\n",
       " 'NOT_sparks',\n",
       " ',',\n",
       " 'but',\n",
       " 'with',\n",
       " 'his',\n",
       " 'affection',\n",
       " 'for',\n",
       " 'Astoria',\n",
       " 'and',\n",
       " 'its',\n",
       " 'people',\n",
       " 'he',\n",
       " 'has',\n",
       " 'given',\n",
       " 'his',\n",
       " 'tale',\n",
       " 'a',\n",
       " 'warm',\n",
       " 'glow',\n",
       " '.',\n",
       " 'A',\n",
       " 'delirious',\n",
       " 'celebration',\n",
       " 'of',\n",
       " 'the',\n",
       " 'female',\n",
       " 'orgasm',\n",
       " '.',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'fascinating',\n",
       " 'to',\n",
       " 'see',\n",
       " 'how',\n",
       " 'Bettany',\n",
       " 'and',\n",
       " 'McDowell',\n",
       " 'play',\n",
       " 'off',\n",
       " 'each',\n",
       " 'other',\n",
       " '.',\n",
       " 'Jose',\n",
       " 'Campanella',\n",
       " 'delivers',\n",
       " 'a',\n",
       " 'loosely',\n",
       " 'autobiographical',\n",
       " 'story',\n",
       " 'brushed',\n",
       " 'with',\n",
       " 'sentimentality',\n",
       " 'but',\n",
       " 'brimming',\n",
       " 'with',\n",
       " 'gentle',\n",
       " 'humor',\n",
       " ',',\n",
       " 'bittersweet',\n",
       " 'pathos',\n",
       " ',',\n",
       " 'and',\n",
       " 'lyric',\n",
       " 'moments',\n",
       " 'that',\n",
       " 'linger',\n",
       " 'like',\n",
       " 'snapshots',\n",
       " 'of',\n",
       " 'memory',\n",
       " '.',\n",
       " 'Neither',\n",
       " 'Parker',\n",
       " 'nor',\n",
       " 'Donovan',\n",
       " 'is',\n",
       " 'a',\n",
       " 'typical',\n",
       " 'romantic',\n",
       " 'lead',\n",
       " ',',\n",
       " 'but',\n",
       " 'they',\n",
       " 'bring',\n",
       " 'a',\n",
       " 'fresh',\n",
       " ',',\n",
       " 'quirky',\n",
       " 'charm',\n",
       " 'to',\n",
       " 'the',\n",
       " 'formula',\n",
       " '.',\n",
       " 'Not',\n",
       " 'NOT_only',\n",
       " 'NOT_are',\n",
       " 'NOT_the',\n",
       " 'NOT_special',\n",
       " 'NOT_effects',\n",
       " 'NOT_and',\n",
       " 'NOT_narrative',\n",
       " 'NOT_flow',\n",
       " 'NOT_much',\n",
       " 'NOT_improved',\n",
       " ',',\n",
       " 'and',\n",
       " 'Daniel',\n",
       " 'Radcliffe',\n",
       " 'more',\n",
       " 'emotionally',\n",
       " 'assertive',\n",
       " 'this',\n",
       " 'time',\n",
       " 'around',\n",
       " 'as',\n",
       " 'Harry',\n",
       " ',',\n",
       " 'but',\n",
       " 'the',\n",
       " 'film',\n",
       " 'conjures',\n",
       " 'the',\n",
       " 'magic',\n",
       " 'of',\n",
       " 'author',\n",
       " 'J.K.',\n",
       " 'Rowling',\n",
       " \"'s\",\n",
       " 'books',\n",
       " '.',\n",
       " 'A',\n",
       " 'must-see',\n",
       " 'for',\n",
       " 'the',\n",
       " 'David',\n",
       " 'Mamet',\n",
       " 'enthusiast',\n",
       " 'and',\n",
       " 'for',\n",
       " 'anyone',\n",
       " 'who',\n",
       " 'appreciates',\n",
       " 'intelligent',\n",
       " ',',\n",
       " 'stylish',\n",
       " 'moviemaking',\n",
       " '.',\n",
       " 'Far',\n",
       " 'more',\n",
       " 'imaginative',\n",
       " 'and',\n",
       " 'ambitious',\n",
       " 'than',\n",
       " 'the',\n",
       " 'trivial',\n",
       " ',',\n",
       " 'cash-in',\n",
       " 'features',\n",
       " 'Nickelodeon',\n",
       " 'has',\n",
       " 'made',\n",
       " 'from',\n",
       " 'its',\n",
       " 'other',\n",
       " 'animated',\n",
       " 'TV',\n",
       " 'series',\n",
       " '.',\n",
       " 'Suffers',\n",
       " 'from',\n",
       " 'the',\n",
       " 'lack',\n",
       " 'of',\n",
       " 'a',\n",
       " 'compelling',\n",
       " 'or',\n",
       " 'comprehensible',\n",
       " 'narrative',\n",
       " '.',\n",
       " 'So',\n",
       " 'unassuming',\n",
       " 'and',\n",
       " 'pure',\n",
       " 'of',\n",
       " 'heart',\n",
       " ',',\n",
       " 'you',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'NOT_help',\n",
       " 'NOT_but',\n",
       " 'NOT_warmly',\n",
       " 'NOT_extend',\n",
       " 'NOT_your',\n",
       " 'NOT_arms',\n",
       " 'NOT_and',\n",
       " 'NOT_yell',\n",
       " 'NOT_`',\n",
       " 'NOT_Safe',\n",
       " '!',\n",
       " \"'\",\n",
       " 'Hoffman',\n",
       " 'notches',\n",
       " 'in',\n",
       " 'the',\n",
       " 'nuances',\n",
       " 'of',\n",
       " 'pain',\n",
       " ',',\n",
       " 'but',\n",
       " 'his',\n",
       " 'smart',\n",
       " ',',\n",
       " 'edgy',\n",
       " 'voice',\n",
       " 'and',\n",
       " 'waddling',\n",
       " 'profile',\n",
       " '-LRB-',\n",
       " 'emphasized',\n",
       " 'here',\n",
       " '-RRB-',\n",
       " 'accent',\n",
       " 'the',\n",
       " 'humor',\n",
       " 'of',\n",
       " 'Wilson',\n",
       " \"'s\",\n",
       " 'plight',\n",
       " ',',\n",
       " 'and',\n",
       " 'that',\n",
       " 'saves',\n",
       " 'his',\n",
       " 'pathos',\n",
       " 'from',\n",
       " 'drippiness',\n",
       " '.',\n",
       " 'People',\n",
       " 'cinema',\n",
       " 'at',\n",
       " 'its',\n",
       " 'finest',\n",
       " '.',\n",
       " 'Dense',\n",
       " 'with',\n",
       " 'characters',\n",
       " 'and',\n",
       " 'contains',\n",
       " 'some',\n",
       " 'thrilling',\n",
       " 'moments',\n",
       " '.',\n",
       " 'Altogether',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'successful',\n",
       " 'as',\n",
       " 'a',\n",
       " 'film',\n",
       " ',',\n",
       " 'while',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " 'being',\n",
       " 'a',\n",
       " 'most',\n",
       " 'touching',\n",
       " 'reconsideration',\n",
       " 'of',\n",
       " 'the',\n",
       " 'familiar',\n",
       " 'masterpiece',\n",
       " '.',\n",
       " '-LRB-',\n",
       " 'D',\n",
       " '-RRB-',\n",
       " 'oes',\n",
       " \"n't\",\n",
       " 'NOT_bother',\n",
       " 'NOT_being',\n",
       " 'NOT_as',\n",
       " 'NOT_cloying',\n",
       " 'NOT_or',\n",
       " 'NOT_preachy',\n",
       " 'NOT_as',\n",
       " 'NOT_equivalent',\n",
       " 'NOT_evangelical',\n",
       " 'NOT_Christian',\n",
       " 'NOT_movies',\n",
       " 'NOT_--',\n",
       " 'NOT_maybe',\n",
       " 'NOT_the',\n",
       " 'NOT_filmmakers',\n",
       " 'NOT_know',\n",
       " 'NOT_that',\n",
       " 'NOT_the',\n",
       " 'NOT_likely',\n",
       " 'NOT_audience',\n",
       " 'NOT_will',\n",
       " 'NOT_already',\n",
       " 'NOT_be',\n",
       " 'NOT_among',\n",
       " 'NOT_the',\n",
       " 'NOT_faithful',\n",
       " '.',\n",
       " 'If',\n",
       " 'director',\n",
       " 'Michael',\n",
       " 'Dowse',\n",
       " 'only',\n",
       " 'superficially',\n",
       " 'understands',\n",
       " 'his',\n",
       " 'characters',\n",
       " ',',\n",
       " 'he',\n",
       " 'does',\n",
       " \"n't\",\n",
       " 'NOT_hold',\n",
       " 'NOT_them',\n",
       " 'NOT_in',\n",
       " 'NOT_contempt',\n",
       " '.',\n",
       " 'If',\n",
       " 'you',\n",
       " 'can',\n",
       " 'stomach',\n",
       " 'the',\n",
       " 'rough',\n",
       " 'content',\n",
       " ',',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'worth',\n",
       " 'checking',\n",
       " 'out',\n",
       " 'for',\n",
       " 'the',\n",
       " 'performances',\n",
       " 'alone',\n",
       " '.',\n",
       " 'Falls',\n",
       " 'neatly',\n",
       " 'into',\n",
       " 'the',\n",
       " 'category',\n",
       " 'of',\n",
       " 'Good',\n",
       " 'Stupid',\n",
       " 'Fun',\n",
       " '.',\n",
       " 'It',\n",
       " 'all',\n",
       " 'adds',\n",
       " 'up',\n",
       " 'to',\n",
       " 'good',\n",
       " 'fun',\n",
       " '.',\n",
       " 'The',\n",
       " 'subtle',\n",
       " 'strength',\n",
       " 'of',\n",
       " '``',\n",
       " 'Elling',\n",
       " \"''\",\n",
       " 'is',\n",
       " 'that',\n",
       " 'it',\n",
       " 'never',\n",
       " 'NOT_loses',\n",
       " 'NOT_touch',\n",
       " 'NOT_with',\n",
       " 'NOT_the',\n",
       " 'NOT_reality',\n",
       " 'NOT_of',\n",
       " 'NOT_the',\n",
       " 'NOT_grim',\n",
       " 'NOT_situation',\n",
       " '.',\n",
       " 'A',\n",
       " 'study',\n",
       " 'in',\n",
       " 'shades',\n",
       " 'of',\n",
       " 'gray',\n",
       " ',',\n",
       " 'offering',\n",
       " 'itself',\n",
       " 'up',\n",
       " 'in',\n",
       " 'subtle',\n",
       " 'plot',\n",
       " 'maneuvers',\n",
       " '...',\n",
       " 'The',\n",
       " 'best',\n",
       " 'revenge',\n",
       " 'may',\n",
       " 'just',\n",
       " 'be',\n",
       " 'living',\n",
       " 'well',\n",
       " 'because',\n",
       " 'this',\n",
       " 'film',\n",
       " ',',\n",
       " 'unlike',\n",
       " 'other',\n",
       " 'Dumas',\n",
       " 'adaptations',\n",
       " ',',\n",
       " 'is',\n",
       " 'far',\n",
       " 'more',\n",
       " 'likened',\n",
       " 'to',\n",
       " 'a',\n",
       " 'treasure',\n",
       " 'than',\n",
       " 'a',\n",
       " 'lengthy',\n",
       " 'jail',\n",
       " 'sentence',\n",
       " '.',\n",
       " 'Ahhhh',\n",
       " '...',\n",
       " 'revenge',\n",
       " 'is',\n",
       " 'sweet',\n",
       " '!',\n",
       " 'Yakusho',\n",
       " 'and',\n",
       " 'Shimizu',\n",
       " '...',\n",
       " 'create',\n",
       " 'engaging',\n",
       " 'characterizations',\n",
       " 'in',\n",
       " 'Imamura',\n",
       " \"'s\",\n",
       " 'lively',\n",
       " 'and',\n",
       " 'enjoyable',\n",
       " 'cultural',\n",
       " 'mix',\n",
       " '.',\n",
       " 'Collateral',\n",
       " 'Damage',\n",
       " 'finally',\n",
       " 'delivers',\n",
       " 'the',\n",
       " 'goods',\n",
       " 'for',\n",
       " 'Schwarzenegger',\n",
       " 'fans',\n",
       " '.',\n",
       " 'This',\n",
       " 'riveting',\n",
       " 'World',\n",
       " 'War',\n",
       " 'II',\n",
       " 'moral',\n",
       " 'suspense',\n",
       " 'story',\n",
       " 'deals',\n",
       " 'with',\n",
       " 'the',\n",
       " 'shadow',\n",
       " 'side',\n",
       " 'of',\n",
       " 'American',\n",
       " 'culture',\n",
       " ':',\n",
       " 'racial',\n",
       " 'prejudice',\n",
       " 'in',\n",
       " 'its',\n",
       " 'ugly',\n",
       " 'and',\n",
       " 'diverse',\n",
       " 'forms',\n",
       " '.',\n",
       " 'It',\n",
       " 'has',\n",
       " 'the',\n",
       " 'charm',\n",
       " 'of',\n",
       " 'the',\n",
       " 'original',\n",
       " 'American',\n",
       " 'road',\n",
       " 'movies',\n",
       " ',',\n",
       " 'feasting',\n",
       " 'on',\n",
       " 'the',\n",
       " 'gorgeous',\n",
       " ',',\n",
       " 'ramshackle',\n",
       " 'landscape',\n",
       " 'of',\n",
       " 'the',\n",
       " 'filmmaker',\n",
       " \"'s\",\n",
       " 'motherland',\n",
       " '.',\n",
       " 'That',\n",
       " 'is',\n",
       " 'a',\n",
       " 'compliment',\n",
       " 'to',\n",
       " 'Kuras',\n",
       " 'and',\n",
       " 'Miller',\n",
       " '.',\n",
       " 'It',\n",
       " 'seems',\n",
       " 'like',\n",
       " 'I',\n",
       " 'have',\n",
       " 'been',\n",
       " 'waiting',\n",
       " 'my',\n",
       " 'whole',\n",
       " 'life',\n",
       " 'for',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'and',\n",
       " 'now',\n",
       " 'I',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'NOT_wait',\n",
       " 'NOT_for',\n",
       " 'NOT_the',\n",
       " 'NOT_sequel',\n",
       " '.',\n",
       " 'The',\n",
       " 'far',\n",
       " 'future',\n",
       " 'may',\n",
       " 'be',\n",
       " 'awesome',\n",
       " 'to',\n",
       " 'consider',\n",
       " ',',\n",
       " 'but',\n",
       " 'from',\n",
       " 'period',\n",
       " 'detail',\n",
       " 'to',\n",
       " 'matters',\n",
       " 'of',\n",
       " 'the',\n",
       " 'heart',\n",
       " ',',\n",
       " 'this',\n",
       " 'film',\n",
       " 'is',\n",
       " 'most',\n",
       " 'transporting',\n",
       " 'when',\n",
       " 'it',\n",
       " 'stays',\n",
       " 'put',\n",
       " 'in',\n",
       " 'the',\n",
       " 'past',\n",
       " '.',\n",
       " 'It',\n",
       " 'inspires',\n",
       " 'a',\n",
       " 'continuing',\n",
       " 'and',\n",
       " 'deeply',\n",
       " 'satisfying',\n",
       " 'awareness',\n",
       " 'of',\n",
       " 'the',\n",
       " 'best',\n",
       " 'movies',\n",
       " 'as',\n",
       " 'monumental',\n",
       " '`',\n",
       " 'picture',\n",
       " 'shows',\n",
       " '.',\n",
       " \"'\",\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'unnerving',\n",
       " 'to',\n",
       " 'see',\n",
       " 'Recoing',\n",
       " \"'s\",\n",
       " 'bizzarre',\n",
       " 'reaction',\n",
       " 'to',\n",
       " 'his',\n",
       " 'unemployment',\n",
       " '.',\n",
       " 'It',\n",
       " 'has',\n",
       " 'charm',\n",
       " 'to',\n",
       " 'spare',\n",
       " ',',\n",
       " 'and',\n",
       " 'unlike',\n",
       " 'many',\n",
       " 'romantic',\n",
       " 'comedies',\n",
       " ',',\n",
       " 'it',\n",
       " 'does',\n",
       " 'not',\n",
       " 'NOT_alienate',\n",
       " 'NOT_either',\n",
       " 'NOT_gender',\n",
       " 'NOT_in',\n",
       " 'NOT_the',\n",
       " 'NOT_audience',\n",
       " '.',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'one',\n",
       " 'heck',\n",
       " 'of',\n",
       " 'a',\n",
       " 'character',\n",
       " 'study',\n",
       " '--',\n",
       " 'not',\n",
       " 'NOT_of',\n",
       " 'NOT_Hearst',\n",
       " 'NOT_or',\n",
       " 'NOT_Davies',\n",
       " 'NOT_but',\n",
       " 'NOT_of',\n",
       " 'NOT_the',\n",
       " 'NOT_unique',\n",
       " 'NOT_relationship',\n",
       " 'NOT_between',\n",
       " 'NOT_them',\n",
       " '.',\n",
       " 'Creepy',\n",
       " ',',\n",
       " 'authentic',\n",
       " 'and',\n",
       " 'dark',\n",
       " '.',\n",
       " 'This',\n",
       " 'disturbing',\n",
       " 'bio-pic',\n",
       " 'is',\n",
       " 'hard',\n",
       " 'to',\n",
       " 'forget',\n",
       " '.',\n",
       " '`',\n",
       " 'They',\n",
       " \"'\",\n",
       " 'begins',\n",
       " 'and',\n",
       " 'ends',\n",
       " 'with',\n",
       " 'scenes',\n",
       " 'so',\n",
       " 'terrifying',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'still',\n",
       " 'stunned',\n",
       " '.',\n",
       " 'The',\n",
       " 'film',\n",
       " 'is',\n",
       " 'a',\n",
       " 'contrivance',\n",
       " ',',\n",
       " 'as',\n",
       " 'artificial',\n",
       " 'as',\n",
       " 'the',\n",
       " 'video',\n",
       " 'games',\n",
       " 'Japanese',\n",
       " 'teens',\n",
       " 'play',\n",
       " 'in',\n",
       " 'a',\n",
       " 'nightclub',\n",
       " 'sequence',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'an',\n",
       " 'enjoyable',\n",
       " 'one',\n",
       " '.',\n",
       " 'Ramsay',\n",
       " ',',\n",
       " 'as',\n",
       " 'in',\n",
       " 'Ratcatcher',\n",
       " ',',\n",
       " 'remains',\n",
       " 'a',\n",
       " 'filmmaker',\n",
       " 'with',\n",
       " 'an',\n",
       " 'acid',\n",
       " 'viewpoint',\n",
       " 'and',\n",
       " 'a',\n",
       " 'real',\n",
       " 'gift',\n",
       " 'for',\n",
       " 'teasing',\n",
       " 'chilly',\n",
       " 'poetry',\n",
       " 'out',\n",
       " 'of',\n",
       " 'lives',\n",
       " 'and',\n",
       " 'settings',\n",
       " 'that',\n",
       " 'might',\n",
       " 'otherwise',\n",
       " 'seem',\n",
       " 'drab',\n",
       " 'and',\n",
       " 'sordid',\n",
       " '.',\n",
       " 'It',\n",
       " 'proves',\n",
       " 'quite',\n",
       " 'compelling',\n",
       " 'as',\n",
       " 'an',\n",
       " 'intense',\n",
       " ',',\n",
       " 'brooding',\n",
       " 'character',\n",
       " 'study',\n",
       " '.',\n",
       " 'One',\n",
       " 'of',\n",
       " 'the',\n",
       " 'smartest',\n",
       " 'takes',\n",
       " 'on',\n",
       " 'singles',\n",
       " 'culture',\n",
       " 'I',\n",
       " \"'ve\",\n",
       " 'seen',\n",
       " 'in',\n",
       " 'a',\n",
       " 'long',\n",
       " 'time',\n",
       " '.',\n",
       " ...]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(flatten([neg_words_process(i) for i in corpus.sents()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
